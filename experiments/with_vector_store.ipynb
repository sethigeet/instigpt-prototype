{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain chromadb sentence-transformers transformers\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc': '{\"code\": \"AE102\", \"title\": \"Data Analysis and Interpretation\", \"department\": {\"name\": \"Aerospace Engineering\", \"slug\": \"aerospace-engineering\"}, \"description\": \"\", \"credits\": 6, \"semester\": [{\"year\": 2023, \"season\": \"autumn\", \"timetable\": []}, {\"year\": 2023, \"season\": \"spring\", \"timetable\": []}], \"tags\": [\"Theory\"]}'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./data/courses.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-en\")\n",
    "# reduce vector length to 2048 and 8192 requires a ton of space/memory\n",
    "embeddings.client.max_seq_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "for item in data:\n",
    "  texts.append(item[\"doc\"])\n",
    "\n",
    "retriever = Chroma.from_texts(texts=texts, embedding=embeddings).as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/TheBloke/openchat_3.5-GGUF/resolve/main/openchat_3.5.Q5_0.gguf\n",
    "# !wget https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q5_0.gguf\n",
    "# !wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/content/mistral-7b-v0.1.Q5_0.gguf\",\n",
    "    temperature=0.5,\n",
    "    n_gpu_layers=40,\n",
    "    n_batch=512,\n",
    "    n_ctx=8192,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom prompts which are supposed to give better results\n",
    "# llama_prompt = \"\"\"\n",
    "# [INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>>\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Context: {context}\n",
    "\n",
    "# Answer: [/INST]\n",
    "# \"\"\"\n",
    "\n",
    "# mistral_prompt = \"\"\"\n",
    "# <s> [INST] You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. [/INST] </s>\n",
    "# [INST] Question: {question}\n",
    "# Context: {context}\n",
    "# Answer: [/INST]\n",
    "# \"\"\"\n",
    "\n",
    "# prompt = mistral_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_llm(llm=llm, retriever=retriever)\n",
    "\n",
    "# To use custom prompts which are supposed to give better results\n",
    "# from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt))\n",
    "# combine_documents_chain = StuffDocumentsChain(\n",
    "#       llm_chain=llm_chain,\n",
    "#       document_variable_name=\"context\",\n",
    "#       document_prompt=PromptTemplate.from_template(\"{page_content}\"),\n",
    "#     )\n",
    "# chain = RetrievalQA(combine_documents_chain=combine_documents_chain, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run({\"query\": \"What is the code of the fluid mechanics course?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/questions.json\") as f:\n",
    "    questions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for question in questions:\n",
    "    answers.append({\"question\": question, \"answer\": chain.run({\"query\": question})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/<MODEL-NAME>.json\", \"w\") as f:\n",
    "    json.dump(answers, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instigpt-prototype-pWnZtMJg-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
